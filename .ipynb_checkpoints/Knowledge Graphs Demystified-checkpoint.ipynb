{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Knowledge Graph from Maintenance Work Order Data\n",
    "\n",
    "In this notebook we are going to construct a simple knowledge graph using Python, and run some queries on the graph in Neo4j. We have broken the notebook into several steps:\n",
    "\n",
    "- Reading in the data\n",
    "- Cleaning the data\n",
    "- Extracting entities via Named Entity Recognition (NER)\n",
    "- Creating relationships between entities via Relation Extraction (RE)\n",
    "- Putting it all together and building a Neo4j graph\n",
    "- Querying the graph in Neo4j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required packages\n",
    "\n",
    "To run this notebook you will need to install the following via pip:\n",
    "\n",
    "- `py2neo`: A library for working with Neo4j in Python.\n",
    "- `gqvis`: Our simple tool for visualising graph queries in Jupyter.\n",
    "- `flair`: A deep learning library for natural language processing. Note this library is quite large (a couple gb I believe). If you don't wish to install this, we have provided non deep-learning based alternatives so you can still follow along.\n",
    "\n",
    "You will also need to have Neo4j installed for the last part of the tutorial. You can download and install Neo4j Desktop [here](https://neo4j.com/).\n",
    "\n",
    "We will be running through the code during the tutorial so there is no need to install anything unless you would also like to try the code out yourself and run some graph queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py2neo in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from py2neo) (2019.9.11)\n",
      "Collecting click==7.0 (from py2neo)\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: colorama in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from py2neo) (0.4.1)\n",
      "Requirement already satisfied: neobolt~=1.7.12 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from py2neo) (1.7.17)\n",
      "Requirement already satisfied: neotime~=1.7.4 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from py2neo) (1.7.4)\n",
      "Requirement already satisfied: prompt_toolkit~=2.0.7 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from py2neo) (2.0.10)\n",
      "Requirement already satisfied: pygments~=2.3.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from py2neo) (2.3.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from py2neo) (2019.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.23 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from py2neo) (1.24.3)\n",
      "Requirement already satisfied: six in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from neotime~=1.7.4->py2neo) (1.12.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from prompt_toolkit~=2.0.7->py2neo) (0.2.5)\n",
      "Installing collected packages: click\n",
      "  Found existing installation: click 8.0.2\n",
      "    Uninstalling click-8.0.2:\n",
      "      Successfully uninstalled click-8.0.2\n",
      "Successfully installed click-7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: celery 5.2.1 has requirement click<9.0,>=8.0, but you'll have click 7.0 which is incompatible.\n",
      "ERROR: black 21.9b0 has requirement click>=7.1.2, but you'll have click 7.0 which is incompatible.\n",
      "WARNING: You are using pip version 19.2.1, however version 22.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gqvis in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: ipython==7.34.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gqvis) (7.34.0)\n",
      "Requirement already satisfied: neo4j<6.0.0,>=5.2.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gqvis) (5.2.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (0.1.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (0.1.6)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (4.3.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (2.0.10)\n",
      "Requirement already satisfied: pygments in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (2.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (0.16.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (4.4.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (40.8.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (0.4.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ipython==7.34.0->gqvis) (0.7.5)\n",
      "Requirement already satisfied: pytz in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from neo4j<6.0.0,>=5.2.0->gqvis) (2019.2)\n",
      "Requirement already satisfied: six in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from traitlets>=4.2->ipython==7.34.0->gqvis) (1.12.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from traitlets>=4.2->ipython==7.34.0->gqvis) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->gqvis) (0.2.5)\n",
      "Requirement already satisfied: parso>=0.5.2 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from jedi>=0.16->ipython==7.34.0->gqvis) (0.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.1, however version 22.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.11.3)\n",
      "Requirement already satisfied: ftfy in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (6.1.1)\n",
      "Requirement already satisfied: mpld3==0.3 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (1.12.1+cu116)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (2.0.0)\n",
      "Requirement already satisfied: gdown==4.4.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (4.4.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.10.1)\n",
      "Requirement already satisfied: pptree in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (3.1)\n",
      "Requirement already satisfied: janome in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.4.2)\n",
      "Requirement already satisfied: gensim>=3.4.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (3.8.1)\n",
      "Requirement already satisfied: conllu>=4.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (4.5.2)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: hyperopt>=0.2.7 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.2.7)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (4.36.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (9.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.22)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (3.5.3)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (4.6.5)\n",
      "Requirement already satisfied: tabulate in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.9.0)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.3.4)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (1.2.13)\n",
      "Requirement already satisfied: wikipedia-api in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (0.5.4)\n",
      "Requirement already satisfied: langdetect in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (4.4.1)\n",
      "Requirement already satisfied: regex in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (2020.4.4)\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from flair) (4.24.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.10.0.2)\n",
      "Requirement already satisfied: six in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown==4.4.0->flair) (1.12.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown==4.4.0->flair) (2.22.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown==4.4.0->flair) (4.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gdown==4.4.0->flair) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->flair) (20.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->flair) (3.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub->flair) (5.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim>=3.4.0->flair) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim>=3.4.0->flair) (1.17.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
      "Requirement already satisfied: py4j in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.10.9.7)\n",
      "Requirement already satisfied: future in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (0.14.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.3.0)\n",
      "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.13.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers>=4.0.0->flair) (0.13.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (1.9.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.1.0)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from smart-open>=1.8.1->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from smart-open>=1.8.1->gensim>=3.4.0->flair) (1.11.9)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim>=3.4.0->flair) (0.3.2)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim>=3.4.0->flair) (1.14.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim>=3.4.0->flair) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from botocore<1.15.0,>=1.14.9->boto3->smart-open>=1.8.1->gensim>=3.4.0->flair) (0.15.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.1, however version 22.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install py2neo\n",
    "!pip install gqvis\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in the data\n",
    "\n",
    "Here is a description of the datasets we are working with in this notebook.\n",
    "\n",
    "First of all, the datasets for the NER model:\n",
    "\n",
    "- `ner_dataset/train.txt`: The dataset we will use to *train* the NER model to predict the entities appearing in each work order.\n",
    "- `ner_dataset/dev.txt`: The dataset we will use to *validate* the quality of the model during training.\n",
    "- `ner_dataset/test.txt`: The dataset we will use to *evaluate* the final performance of the NER model after training.\n",
    "\n",
    "We also have three datasets for the Relation Extraction (RE) model:\n",
    "\n",
    "- `re_dataset/train.csv`\n",
    "- `re_dataset/dev.csv`\n",
    "- `re_dataset/test.csv`\n",
    "\n",
    "We are going to be building a knowledge graph on a small sample set of work orders. This will not be seen by the NER or RE models prior to constructing the graph - the idea is to get our models to run *inference* over this dataset to automatically predict the entities, and relationships between the entities, to build a graph.\n",
    "\n",
    "- `sample_work_orders.csv`: A csv file containing a set of work orders.\n",
    "\n",
    "Here is an example of what the first few rows of each dataset look like:\n",
    "\n",
    "![alt text](images/example-data.png \"Example datasets\")\n",
    "\n",
    "We are using the simple `csv` library to read in the data, though this can also be done using `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the data\n",
    "\n",
    "Let's start by inspecting the `sample_work_orders.csv` CSV dataset. This is the dataset we will be building the graph from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('StartDate', '10/07/2005'), ('FLOC', '1234.1.1'), ('ShortText', 'repair cracked hyd tank')])\n",
      "OrderedDict([('StartDate', '14/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine wont start')])\n",
      "OrderedDict([('StartDate', '17/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c blowing hot air')])\n",
      "OrderedDict([('StartDate', '20/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engin u/s')])\n",
      "OrderedDict([('StartDate', '21/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'fix engine')])\n",
      "OrderedDict([('StartDate', '22/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump service')])\n",
      "OrderedDict([('StartDate', '23/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leak')])\n",
      "OrderedDict([('StartDate', '24/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'fix leak on pump')])\n",
      "OrderedDict([('StartDate', '25/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine not running')])\n",
      "OrderedDict([('StartDate', '26/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine has problems starting')])\n",
      "OrderedDict([('StartDate', '27/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump fault')])\n",
      "OrderedDict([('StartDate', '28/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leaking')])\n",
      "OrderedDict([('StartDate', '29/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c not working')])\n",
      "OrderedDict([('StartDate', '30/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c broken')])\n"
     ]
    }
   ],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "work_order_file = \"data/sample_work_orders.csv\"\n",
    "\n",
    "# A simple function to read in a csv file and return a list,\n",
    "# where each element in the list is a dictionary of {heading : value}\n",
    "def load_csv(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "        \n",
    "work_order_data = load_csv(work_order_file)\n",
    "\n",
    "for row in work_order_data:\n",
    "    print(row)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning the data\n",
    "\n",
    "TODO: Probably best to have a simple model for cleaning the data here. I don't think we need to go into too much detail about it, but it would be nice to have this as the first step so things like 'hyd pump' get corrected before running NER/RE. We could mention Lexiclean here\n",
    "\n",
    "I could just chuck the lexicon cleaner thing from the masterclass here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition\n",
    "\n",
    "Our first task is to extract the entities in the short text descriptions and construct nodes from those entities. This is how we are able to unlock the knowledge captured within the short text and combine it with the structured fields.\n",
    "\n",
    "![alt text](images/extracting-entities-v2.png \"Extracting entities\")\n",
    "\n",
    "## 3.1. Loading and inspecting the data\n",
    "\n",
    "Let's start by defining some functions for loading the CONLL-formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NER_DATASET_PATH = \"data/ner_dataset\"\n",
    "\n",
    "\n",
    "def to_conll_document(s: str):\n",
    "    \"\"\"Create a ConllDocument from a string as it appears\n",
    "    in a Conll-formatted file.\n",
    "\n",
    "    Args:\n",
    "        s (str): A string, separated by newlines, where each\n",
    "        line is a token, then a comma and space, then a label.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dict of tokens and labels.\n",
    "    \"\"\"\n",
    "    tokens, labels = [], []\n",
    "    for line in s.split(\"\\n\"):\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        token, label = line.split()\n",
    "\n",
    "        tokens.append(token)\n",
    "        labels.append(label)\n",
    "    return {'tokens': tokens, 'labels': labels}\n",
    "\n",
    "\n",
    "def load_conll_dataset(filename: str) -> list:\n",
    "    \"\"\"Load a list of documents from the given CONLL-formatted dataset.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to load from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of documents, where each document is a dict of tokens and labels.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        docs = f.read().split(\"\\n\\n\")\n",
    "        for d in docs:\n",
    "            if len(d) == 0:\n",
    "                continue\n",
    "            document = to_conll_document(d)\n",
    "            documents.append(document)\n",
    "    print(f\"Loaded {len(documents)} documents from {filename}.\")\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the first row of our training dataset to make sure it loads OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3200 documents from data/ner_dataset\\train.txt.\n",
      "{'tokens': ['ram', 'on', 'cup', 'rod', 'support', 'broken'], 'labels': ['B-Item', 'B-Location', 'B-Item', 'B-Item', 'I-Item', 'B-Observation']}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_conll_dataset(os.path.join(NER_DATASET_PATH, 'train.txt'))\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define an abstract base class for NER Models\n",
    "\n",
    "Seeing as we would like to be able to work with a range of NER models, it's a good idea to create an 'abstract base class' to represent an NER model. This way, we can create classes for our NER models that inherit from this base class. Every model we create must have these four functions:\n",
    "\n",
    "- `train`: Train the model on the datasets in the given path.\n",
    "- `inference`: Run inference over the given list of sentences.\n",
    "- `load`: Load the model from the given path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Abstract base class for the NER Model. \"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class NERModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, conll_datasets_path: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, raw_sents: list):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, model_path):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Define a Flair-based NER Model class\n",
    "\n",
    "In this tutorial we will use [Flair](https://github.com/flairNLP/flair), which simplifies the process of building a deep learning model for a variety of NLP tasks.\n",
    "\n",
    "The code below is a class representing a `FlairNERModel`, which is based on the `NERModel` class above. It has the same four methods, i.e `train()`, `inference()`, `load()`, and `save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mwo2kg_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5a8ebaf04adf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# TODO: Get rid of ConllDataset/ConllDocument and just use lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m from mwo2kg_datasets import (\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mConllDataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mConllDocument\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mwo2kg_datasets'"
     ]
    }
   ],
   "source": [
    "\"\"\"A Flair-based Named Entity Recognition model. Learns to predict entity\n",
    "classes via deep learning.\"\"\"\n",
    "\n",
    "\n",
    "# TODO: Tidy up, fix this code as it does not work atm in this notebook\n",
    "\n",
    "\n",
    "import os\n",
    "import flair\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import (\n",
    "    StackedEmbeddings,\n",
    "    FlairEmbeddings,\n",
    ")\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from typing import List\n",
    "from flair.visual.training_curves import Plotter\n",
    "import torch\n",
    "\n",
    "\n",
    "# TODO: Get rid of ConllDataset/ConllDocument and just use lists\n",
    "from mwo2kg_datasets import (\n",
    "    ConllDataset,\n",
    "    ConllDocument,\n",
    ")\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Check whether CUDA is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    flair.device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    flair.device = torch.device(\"cpu\")\n",
    "print(\"Device:\", flair.device)\n",
    "\n",
    "\n",
    "class FlairNERModel(NERModel):\n",
    "\n",
    "    model_name: str = \"Flair\"\n",
    "\n",
    "    \"\"\"A Flair-based Named Entity Recognition model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FlairNERModel, self).__init__()\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\" Train the Flair model on the given conll datasets.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The folder containing the\n",
    "              train, dev and text CONLL-formatted datasets.\n",
    "            trained_model_path (os.path): The folder to save the trained\n",
    "              model to.\n",
    "        \"\"\"\n",
    "\n",
    "        columns = {0: \"text\", 1: \"ner\"}\n",
    "        corpus: Corpus = ColumnCorpus(\n",
    "            datasets_path,\n",
    "            columns,\n",
    "            train_file=\"train.txt\",\n",
    "            dev_file=\"dev.txt\",\n",
    "            test_file=\"test.txt\",\n",
    "        )\n",
    "        label_dict = corpus.make_label_dictionary(label_type=\"ner\")\n",
    "\n",
    "        # Train the sequence tagger\n",
    "        embedding_types = [\n",
    "            FlairEmbeddings(\"mix-forward\"),\n",
    "            FlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "\n",
    "        embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "        tagger = SequenceTagger(\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            embeddings=embeddings,\n",
    "            tag_dictionary=label_dict,\n",
    "            tag_type=\"ner\",\n",
    "            use_crf=True,\n",
    "        )\n",
    "\n",
    "        trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "        sm = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            sm = \"gpu\"\n",
    "        trainer.train(\n",
    "            trained_model_path,\n",
    "            learning_rate=0.1,\n",
    "            mini_batch_size=32,\n",
    "            max_epochs=10,\n",
    "            embeddings_storage_mode=sm,\n",
    "        )\n",
    "\n",
    "        plotter = Plotter()\n",
    "        plotter.plot_weights(os.path.join(trained_model_path, \"weights.txt\"))\n",
    "\n",
    "        self.load(os.path.join(trained_model_path, 'final-model.pt'))\n",
    "\n",
    "    def inference(self, raw_sents: list) -> ConllDataset:\n",
    "        \"\"\"Run the inference on a given list of short texts.\n",
    "\n",
    "        Args:\n",
    "            raw_sents (list): The list of raw sents to run the inference on.\n",
    "\n",
    "        Returns:\n",
    "            ConllDataset: The ConllDataset of preds.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model has not yet been trained.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"The KGC Model has not yet been trained. \"\n",
    "                \"Please train this Flair model before proceeding.\"\n",
    "            )\n",
    "\n",
    "        preds_dataset = ConllDataset()\n",
    "\n",
    "        for i, tokens in enumerate(raw_sents):\n",
    "            labels = self._tag_sentence(tokens)\n",
    "            doc = ConllDocument(tokens, labels)\n",
    "            preds_dataset.add_document(doc)\n",
    "\n",
    "        return preds_dataset\n",
    "\n",
    "    def load(self, model_path: str):\n",
    "        \"\"\"Load the model from the specified path.\n",
    "\n",
    "        Args:\n",
    "            model_path (os.path): The path to load.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the path does not exist i.e. model not yet trained.\n",
    "        \"\"\"\n",
    "        self.model = SequenceTagger.load(model_path)\n",
    "\n",
    "    def _tag_sentence(self, sentence: List[str]) -> List[str]:\n",
    "        \"\"\"Tag the given sentence (list of tokens) via the model.\n",
    "\n",
    "        Args:\n",
    "            sentence (List[str]): A list of tokens.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of labels.\n",
    "        \"\"\"\n",
    "        sentence_obj = Sentence(sentence, use_tokenizer=False)\n",
    "        self.model.predict(sentence_obj)\n",
    "        labels = [\"O\"] * len(sentence)\n",
    "\n",
    "        for entity in sentence_obj.get_spans(\"ner\"):\n",
    "            for i, token in enumerate(entity):\n",
    "                label = entity.get_label(\"ner\").value\n",
    "                prefix = \"B-\" if i == 0 else \"I-\"\n",
    "                \n",
    "                # Token idx starts from 1 in Flair.\n",
    "                labels[token.idx - 1] = prefix + label\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Define a DictionaryNERModel class\n",
    "\n",
    "If you are not able to use the Flair library, here is a simple model you can use to extract the entities, albeit with a much weaker performance. This one scans the training data, builds a mapping between each phrase (one or more tokens in a row) and the most common entity type associated with that phrase, then uses that entity type as the prediction when seeing that token in the test data.\n",
    "\n",
    "The model is super simple, so we won't show the code here, but feel free to have a look under `helpers/DictionaryNERModel.py` if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import DictionaryNERModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Training the model\n",
    "\n",
    "Depending on whether you are using Flair or the DictionaryNERModel, you can run one of the cells below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Using Flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained the Flair-based model and have uploaded the model onto Huggingface. The following code will download that model and load the weights, so there is no need for you to train the model yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FlairNERModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-95be3cd0c64f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlairNERModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#m.train(NER_DATASET_PATH, os.path.join('models/ner_models')) # Uncomment to train manually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nlp-tlp/mwo-ner-test\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# TODO: Replace with load_pretrained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FlairNERModel' is not defined"
     ]
    }
   ],
   "source": [
    "m = FlairNERModel()\n",
    "#m.train(NER_DATASET_PATH, 'models/ner_models/flair') # Uncomment to train manually\n",
    "m.load(\"nlp-tlp/mwo-ner-test\") # TODO: Replace with load_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Using the DictionaryNERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loaded 3200 documents from data/ner_dataset\\train.txt.\n",
      "Loaded 401 documents from data/ner_dataset\\dev.txt.\n"
     ]
    }
   ],
   "source": [
    "m = DictionaryNERModel()\n",
    "m.train(NER_DATASET_PATH, 'models/ner_models/dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Running inference on unseen sentences\n",
    "\n",
    "The next step is to use our trained model to infer the entity type of each entity appearing in a list of previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['a/c', 'not', 'working'], 'labels': ['B-Item', 'B-Observation', 'I-Observation']}\n"
     ]
    }
   ],
   "source": [
    "tagged_bio_sents = []\n",
    "\n",
    "sentences = []\n",
    "for row in work_order_data:\n",
    "    sentences.append(row[\"ShortText\"].split()) # We must 'tokenise' the sentence first, i.e. split into words\n",
    "\n",
    "tagged_bio_sents = m.inference(sentences)\n",
    "\n",
    "# Print an example tagged sentence\n",
    "print(tagged_bio_sents[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Converting the BIO format to the \"Mention\"-based format\n",
    "\n",
    "The BIO-based format above has one key downside - it is not good for representing 'phrases' of more than one token in length. This makes it difficult to work with for future steps, such as constructing nodes from the entities and running relation extraction. In light of this, we will now convert the BIO-formatted predictions into Mention format, i.e. go from this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'labels': ['B-Item', 'B-Observation', 'I-Observation']}\n",
    "    \n",
    "To this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'mentions': [\n",
    "         {'start': 0, 'labels': ['Item'], 'end': 1},\n",
    "         {'start': 1, 'labels': ['Observation'], 'end': 3}]}\n",
    "    \n",
    "Note that this format is also able to now support multiple labels per mention (though we will only be using single labels for simplicity). Researchers use this format for **entity typing**, which is similar to NER but with >= 1 label per mention.\n",
    "\n",
    "This step is just a bit of data wrangling - here we have defined a helper function to convert a BIO-tagged sentence into a Mention-tagged sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"tokens\": [\n",
      "  \"a/c\",\n",
      "  \"not\",\n",
      "  \"working\"\n",
      " ],\n",
      " \"mentions\": [\n",
      "  {\n",
      "   \"start\": 0,\n",
      "   \"labels\": [\n",
      "    \"Item\"\n",
      "   ],\n",
      "   \"end\": 1\n",
      "  },\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"labels\": [\n",
      "    \"Observation\"\n",
      "   ],\n",
      "   \"end\": 3\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def _bio_to_mention(conll_doc: dict):\n",
    "    \"\"\"Return a Mention-format representation of a BIO-formatted\n",
    "    tagged sentence.\n",
    "\n",
    "    Args:\n",
    "        conll_doc (ConllDocument): The doc to convert to redcoat.\n",
    "        doc_idx (int): The id of the doc, necessary to create a Redcoat doc.\n",
    "\n",
    "    Returns:\n",
    "        dict: A mention-formatted dict created from the conll_doc.\n",
    "    \"\"\"\n",
    "    tokens = conll_doc[\"tokens\"]\n",
    "    labels = conll_doc[\"labels\"]\n",
    "    mentions_list = []\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "    label = None\n",
    "    for i, (token, label) in enumerate(\n",
    "        zip(tokens, labels)\n",
    "    ):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if len(mentions_list) > 0:\n",
    "                mentions_list[-1][\"end\"] = i\n",
    "            mentions_list.append({\"start\": i, \"labels\": [label[2:]]})\n",
    "        elif label == \"O\" and len(mentions_list) > 0:\n",
    "            mentions_list[-1][\"end\"] = i\n",
    "        if len(mentions_list) == 0:\n",
    "            continue\n",
    "        if i == (len(tokens) - 1) and \"end\" not in mentions_list[-1]:\n",
    "            mentions_list[-1][\"end\"] = i + 1\n",
    "    return {'tokens': tokens, 'mentions': mentions_list}\n",
    "\n",
    "\n",
    "# For each BIO tagged sentence in tagged_sents, convert it to the mention-based\n",
    "# representation\n",
    "tagged_sents_m = []\n",
    "for doc in tagged_bio_sents:\n",
    "    mention_doc = _bio_to_mention(doc)\n",
    "    tagged_sents_m.append(mention_doc)\n",
    "\n",
    "# Let's print our example sentence again, this time with the mention-based\n",
    "# representation.\n",
    "# We'll use json.dumps to make it a bit easier to read.\n",
    "print(json.dumps(tagged_sents_m[12],indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extracting relations between the entities via Relation Extraction\n",
    "\n",
    "We have extracted the entities appearing in each work order. The next step is to extract the relationships between those entities. We can do this using Relation Extraction.\n",
    "\n",
    "![alt text](images/building-relations.png \"Building relations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Loading and inspecting the data\n",
    "\n",
    "Let's take a look again at the RE dataset we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " [\n",
      "  \"broken\",\n",
      "  \"rod support\",\n",
      "  \"Observation\",\n",
      "  \"Item\",\n",
      "  \"rod support broken\",\n",
      "  \"0\",\n",
      "  \"1\",\n",
      "  \"O\"\n",
      " ],\n",
      " [\n",
      "  \"rod support\",\n",
      "  \"broken\",\n",
      "  \"Item\",\n",
      "  \"Observation\",\n",
      "  \"rod support broken\",\n",
      "  \"1\",\n",
      "  \"0\",\n",
      "  \"HAS_OBSERVATION\"\n",
      " ],\n",
      " [\n",
      "  \"broken\",\n",
      "  \"cup\",\n",
      "  \"Observation\",\n",
      "  \"Item\",\n",
      "  \"cup rod support broken\",\n",
      "  \"0\",\n",
      "  \"2\",\n",
      "  \"O\"\n",
      " ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "RE_DATASET_PATH = \"data/re_dataset\"\n",
    "\n",
    "\n",
    "def load_re_dataset(filename: str) -> list:\n",
    "    \"\"\"Load the Relation Extraction dataset into a list.\n",
    "        \n",
    "    Args:\n",
    "        filename (str): The name of the file to load.\n",
    "    \"\"\"\n",
    "    re_data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for row in f:\n",
    "            re_data.append(row.strip().split(','))\n",
    "    return re_data\n",
    "\n",
    "train_dataset = load_re_dataset(os.path.join(RE_DATASET_PATH, 'train.csv'))\n",
    "\n",
    "print(json.dumps(train_dataset[0:3], indent=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret this as follows:\n",
    " - 'broken': entity 1\n",
    " - 'rod support': entity 2\n",
    " - 'Observation': label of entity 1\n",
    " - 'Item': label of entity 2\n",
    " - 'rod support broken': The text between 'broken' and 'rod support', inclusive\n",
    " - '0': The position of entity 1\n",
    " - '1': The position of entity 2\n",
    " - 'O': The relation type. \"O\" means no relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Define the Abstract Base Class\n",
    "\n",
    "We are going to see two different RE models, so let's define an abstract base class again just like we did for the NER models. Just like the NER model, we have four functions:\n",
    "\n",
    "- `inference`: Given a row (as above, but without the last column), predict the given relation type (\"O\" if no relation).\n",
    "- `train`: Train the model on the files in the given dataset path.\n",
    "- `load`: Load the model from the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, row: list) -> str:\n",
    "        pass        \n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, re_datasets_path: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, model_path: str):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Define the Flair model\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A dictionary-based KGC model. Can be used as an alternative\n",
    "to Flair, which is cumbersome to run and install.\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import flair\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.embeddings import (\n",
    "    PooledFlairEmbeddings,\n",
    "    DocumentRNNEmbeddings,\n",
    ")\n",
    "from flair.data import Sentence\n",
    "from typing import List\n",
    "from flair.models import TextClassifier\n",
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "import torch\n",
    "\n",
    "MAX_EPOCHS = 1\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Check whether CUDA is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    flair.device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    flair.device = torch.device(\"cpu\")\n",
    "print(\"Device:\", flair.device)\n",
    "\n",
    "\n",
    "class FlairREModel(REModel):\n",
    "\n",
    "    \"\"\"The Flair-based RE model.\"\"\"\n",
    "\n",
    "    model_name: str = \"Flair\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FlairREModel, self).__init__()\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\"Train the Flair RE model on the given CSV datasets.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The path containing the train and dev\n",
    "               datasets.\n",
    "            trained_model_path (os.path): The path to save the trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        column_name_map = {\n",
    "            0: \"text\",\n",
    "            1: \"text\",\n",
    "            2: \"text\",\n",
    "            3: \"text\",\n",
    "            4: \"text\",\n",
    "            7: \"label_relation\",\n",
    "        }\n",
    "\n",
    "        # Define corpus, labels, word embeddings, doc embeddings\n",
    "        corpus = CSVClassificationCorpus(\n",
    "            datasets_path,\n",
    "            column_name_map,\n",
    "            delimiter=\",\",\n",
    "            label_type=\"relation\",\n",
    "        )\n",
    "\n",
    "        label_dict = corpus.make_label_dictionary(label_type=\"relation\")\n",
    "\n",
    "        word_embeddings = [\n",
    "            PooledFlairEmbeddings(\"mix-forward\"),\n",
    "            PooledFlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "\n",
    "        document_embeddings = DocumentRNNEmbeddings(\n",
    "            word_embeddings, hidden_size=HIDDEN_SIZE\n",
    "        )\n",
    "\n",
    "        # Initialise sequence tagger\n",
    "        tagger = TextClassifier(\n",
    "            document_embeddings,\n",
    "            label_dictionary=label_dict,\n",
    "            label_type=\"relation\",\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "        sm = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            sm = \"gpu\"\n",
    "        \n",
    "        # Start training\n",
    "        trainer.train(\n",
    "            trained_model_path,\n",
    "            learning_rate=0.1,\n",
    "            mini_batch_size=32,\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            patience=3,\n",
    "            embeddings_storage_mode=sm,\n",
    "        )\n",
    "\n",
    "        self.load(os.path.join(trained_model_path, 'final-model.pt'))\n",
    "\n",
    "    def load(self, model_path: str):\n",
    "        \"\"\"Load the chunked frequency dict from the given folder.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The filename containing the model.\n",
    "               Can also be the name of a repo on Huggingface.\n",
    "        \"\"\"\n",
    "        TextClassifier.load(model_path)\n",
    "\n",
    "    def inference(self, row: list) -> str:\n",
    "        \"\"\"Run the inference over the given document.\n",
    "\n",
    "        Args:\n",
    "            row (list): The row to predict the relation of.\n",
    "\n",
    "        Returns:\n",
    "            str: The relation type.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = Sentence(\" \".join(rel[:5]))\n",
    "        label = \"O\"\n",
    "        self.model.predict(s)\n",
    "        if len(s.labels) > 0:\n",
    "            label = str(s.labels[0].value)\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Define a 'SimpleMWO' RE model\n",
    "\n",
    "Because maintenance work orders are very short (5-7 words typically), generally speaking we can create a useful knowledge graph by simply linking each Item entity in the work order and each other entity in that work order. For example:\n",
    "\n",
    "    replace pump\n",
    "    \n",
    "We can say the \"pump\" entity `HAS_ACTIVITY` \"replace\". Likewise for the following:\n",
    "\n",
    "    fix air conditioner , not working\n",
    "    \n",
    "We can say that \"air conditioner\" `HAS_ACTIVITY` \"fix\", and `HAS_OBSERVATION` \"not working\".\n",
    "\n",
    "This is not a foolproof method, though - it is a heuristic, i.e. a rule-based method designed to exploit a pattern in the data. For creating this specific type of knowledge graph, though, it works quite well, and thus we can define a model to use this heuristic as a weaker alternative to a deep learning model.\n",
    "\n",
    "Just like the dictionary-based NER model, the model is super simple, so we won't show the code here, but feel free to have a look under `helpers/SimpleMWOREModel.py` if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import SimpleMWOREModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example output from the model. Note we have removed the last column, which our model is predicting now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAS_OBSERVATION'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = SimpleMWOREModel()\n",
    "\n",
    "r.inference([\n",
    "  \"rod support\",\n",
    "  \"broken\",\n",
    "  \"Item\",\n",
    "  \"Observation\",\n",
    "  \"rod support broken\",\n",
    "  \"1\",\n",
    "  \"0\"\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Train the model/load the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pretrained Flair RE model from Huggingface.\n",
    "\n",
    "(or alternatively you can train it yourself by uncommenting the train line, and commenting the load line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 17:18:29,714 Reading data from data\\re_dataset\n",
      "2022-11-21 17:18:29,715 Train: data\\re_dataset\\train.csv\n",
      "2022-11-21 17:18:29,715 Dev: data\\re_dataset\\dev.csv\n",
      "2022-11-21 17:18:29,716 Test: data\\re_dataset\\test.csv\n",
      "2022-11-21 17:18:29,808 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24804it [00:04, 6187.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 17:18:33,821 Dictionary created for label 'relation' with 13 values: O (seen 15398 times), HAS_ACTIVITY (seen 2825 times), HAS_OBSERVATION (seen 2174 times), APPEARS_WITH (seen 1982 times), HAS_LOCATION (seen 1556 times), HAS_CONSUMABLE (seen 334 times), HAS_SPECIFIER (seen 173 times), HAS_AGENT (seen 143 times), HAS_CARDINALITY (seen 114 times), HAS_ATTRIBUTE (seen 76 times), HAS_TIME (seen 25 times), HAS_EVENT (seen 4 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flair\\trainers\\trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  \"There should be no best model saved at epoch 1 except there \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 17:18:34,783 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:18:34,784 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=256, out_features=13, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): PooledFlairEmbeddings(\n",
      "        (context_embeddings): FlairEmbeddings(\n",
      "          (lm): LanguageModel(\n",
      "            (drop): Dropout(p=0.25, inplace=False)\n",
      "            (encoder): Embedding(275, 100)\n",
      "            (rnn): LSTM(100, 2048)\n",
      "            (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): PooledFlairEmbeddings(\n",
      "        (context_embeddings): FlairEmbeddings(\n",
      "          (lm): LanguageModel(\n",
      "            (drop): Dropout(p=0.25, inplace=False)\n",
      "            (encoder): Embedding(275, 100)\n",
      "            (rnn): LSTM(100, 2048)\n",
      "            (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=8192, out_features=8192, bias=True)\n",
      "    (rnn): GRU(8192, 256, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-11-21 17:18:34,785 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:18:34,786 Corpus: \"Corpus: 24804 train + 3310 dev + 3234 test sentences\"\n",
      "2022-11-21 17:18:34,787 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:18:34,788 Parameters:\n",
      "2022-11-21 17:18:34,789  - learning_rate: \"0.100000\"\n",
      "2022-11-21 17:18:34,789  - mini_batch_size: \"32\"\n",
      "2022-11-21 17:18:34,790  - patience: \"3\"\n",
      "2022-11-21 17:18:34,791  - anneal_factor: \"0.5\"\n",
      "2022-11-21 17:18:34,792  - max_epochs: \"1\"\n",
      "2022-11-21 17:18:34,793  - shuffle: \"True\"\n",
      "2022-11-21 17:18:34,794  - train_with_dev: \"False\"\n",
      "2022-11-21 17:18:34,795  - batch_growth_annealing: \"False\"\n",
      "2022-11-21 17:18:34,796 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:18:34,797 Model training base path: \"models\\re_models\\flair\"\n",
      "2022-11-21 17:18:34,798 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:18:34,800 Device: cuda:0\n",
      "2022-11-21 17:18:34,801 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:18:34,802 Embeddings storage mode: gpu\n",
      "2022-11-21 17:18:34,803 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:18:34,804 train mode resetting embeddings\n",
      "2022-11-21 17:18:34,806 train mode resetting embeddings\n",
      "2022-11-21 17:18:49,436 epoch 1 - iter 77/776 - loss 0.05936103 - samples/sec: 172.71 - lr: 0.100000\n",
      "2022-11-21 17:19:06,118 epoch 1 - iter 154/776 - loss 0.04923357 - samples/sec: 150.94 - lr: 0.100000\n",
      "2022-11-21 17:19:20,186 epoch 1 - iter 231/776 - loss 0.04102636 - samples/sec: 179.60 - lr: 0.100000\n",
      "2022-11-21 17:19:35,234 epoch 1 - iter 308/776 - loss 0.03360744 - samples/sec: 169.31 - lr: 0.100000\n",
      "2022-11-21 17:19:50,469 epoch 1 - iter 385/776 - loss 0.02840906 - samples/sec: 167.23 - lr: 0.100000\n",
      "2022-11-21 17:20:05,045 epoch 1 - iter 462/776 - loss 0.02472141 - samples/sec: 173.24 - lr: 0.100000\n",
      "2022-11-21 17:20:18,593 epoch 1 - iter 539/776 - loss 0.02205730 - samples/sec: 186.63 - lr: 0.100000\n",
      "2022-11-21 17:20:32,494 epoch 1 - iter 616/776 - loss 0.02001808 - samples/sec: 181.79 - lr: 0.100000\n",
      "2022-11-21 17:20:46,719 epoch 1 - iter 693/776 - loss 0.01831902 - samples/sec: 179.24 - lr: 0.100000\n",
      "2022-11-21 17:21:02,159 epoch 1 - iter 770/776 - loss 0.01691112 - samples/sec: 163.26 - lr: 0.100000\n",
      "2022-11-21 17:21:03,378 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:21:03,380 EPOCH 1 done: loss 0.0168 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 104/104 [00:17<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 17:21:20,435 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 17:21:20,455 DEV : loss 0.0031679796520620584 - f1-score (micro avg)  0.9087\n",
      "2022-11-21 17:21:21,127 BAD EPOCHS (no improvement): 0\n",
      "2022-11-21 17:21:21,129 saving best model\n",
      "2022-11-21 17:21:24,305 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:21:24,307 loading file models\\re_models\\flair\\best-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 102/102 [00:16<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 17:21:42,423 Evaluating as a multi-label problem: False\n",
      "2022-11-21 17:21:42,441 0.9979\t0.8154\t0.8975\t0.9326\n",
      "2022-11-21 17:21:42,443 \n",
      "Results:\n",
      "- F-score (micro) 0.8975\n",
      "- F-score (macro) 0.8786\n",
      "- Accuracy 0.9326\n",
      "\n",
      "By class:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "HAS_OBSERVATION     1.0000    1.0000    1.0000       313\n",
      "   HAS_ACTIVITY     1.0000    1.0000    1.0000       279\n",
      "   HAS_LOCATION     1.0000    1.0000    1.0000       238\n",
      "   APPEARS_WITH     0.6667    0.0183    0.0357       218\n",
      " HAS_CONSUMABLE     1.0000    1.0000    1.0000        59\n",
      "      HAS_AGENT     1.0000    1.0000    1.0000        21\n",
      "  HAS_SPECIFIER     1.0000    1.0000    1.0000        15\n",
      "  HAS_ATTRIBUTE     1.0000    1.0000    1.0000        12\n",
      "HAS_CARDINALITY     1.0000    1.0000    1.0000        10\n",
      "       HAS_TIME     1.0000    0.6000    0.7500         5\n",
      "\n",
      "      micro avg     0.9979    0.8154    0.8975      1170\n",
      "      macro avg     0.9667    0.8618    0.8786      1170\n",
      "   weighted avg     0.9379    0.8154    0.8193      1170\n",
      "\n",
      "2022-11-21 17:21:42,443 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 17:21:42,444 loading file models/re_models/flair\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'models/re_models/flair'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a591f9ce66c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mre_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlairREModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mre_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRE_DATASET_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"models/re_models/flair\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Uncomment to train manually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# TODO: Load from huggingface\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-0cada447c4f6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, datasets_path, trained_model_path)\u001b[0m\n\u001b[0;32m    103\u001b[0m         )\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-0cada447c4f6>\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    112\u001b[0m                \u001b[0mCan\u001b[0m \u001b[0malso\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrepo\u001b[0m \u001b[0mon\u001b[0m \u001b[0mHuggingface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mTextClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flair\\nn\\model.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, model_path)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;31m# to load models on some Mac/Windows setups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;31m# see https://github.com/zalandoresearch/flair/issues/351\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_big_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flair\\file_utils.py\u001b[0m in \u001b[0;36mload_big_file\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \"\"\"\n\u001b[0;32m     33\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loading file {f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;31m# mmap seems to be much more memory efficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mbf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACCESS_READ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'models/re_models/flair'"
     ]
    }
   ],
   "source": [
    "re_model = FlairREModel()\n",
    "re_model.train(RE_DATASET_PATH, \"models/re_models/flair\") # Uncomment to train manually\n",
    "\n",
    "# TODO: Load from huggingface\n",
    "# re_model.load('nlp-tlp/mwo-re')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Inference\n",
    "\n",
    "Now we have our RE model, let's run it over the MWO dataset to extract the relationships between the entities.\n",
    "\n",
    "We need our data to be in the same format as required by the model, i.e. a list of rows where each row has five columns (entity 1, entity 2, etc), just like the training data used to train the model.\n",
    "\n",
    "So before we can run RE, we need to 'wrangle' our data again to get it into the right format. Here is a helper function to transform our mention-based entity format of a single document into a list of potential relationships between each entity and each other entity in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Combining NER+RE\n",
    "\n",
    "TODO: Do something like this but using RE instead of simple Item -> everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is old code from the master class\n",
    "\n",
    "triples = []\n",
    "\n",
    "for row in normalised_work_order_entities:\n",
    "    for (ngram, entity_class) in row:\n",
    "        if entity_class != \"item\": continue\n",
    "            \n",
    "        # If this entity is an item, link it to all other entities in the work order       \n",
    "             \n",
    "        for (other_ngram, other_entity_class) in row:   \n",
    "            if ngram == other_ngram: continue # Don't link items to themselves                \n",
    "\n",
    "            relation_type = other_entity_class.upper()                \n",
    "            triples.append(((ngram, entity_class), \"HAS_%s\" % relation_type, (other_ngram, other_entity_class)))\n",
    "        \n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating the graph\n",
    "\n",
    "NOTE: This is still old code, but it probably works well enough to just reuse it here too. I have better ways to do this now (using IMPORT rather than doing it directly via py2neo functions) but this is probably good enough. At the moment it's missing the `triples`, that can be built in section 4.\n",
    "\n",
    "\n",
    "Now that we have our nodes and relations we can go ahead and build the Neo4J graph.\n",
    "\n",
    "To do this we are going to use py2neo, a Python library for interacting with Neo4J.\n",
    "\n",
    "There are also a couple of other ways to do this - you can either use Neo4J and run Cypher queries to insert each node and relation, or use the APOC library to import a list of nodes from a CSV file. I find Python to be the simplest way, however.\n",
    "\n",
    "> Before proceeding, make sure you have created a new graph in Neo4j and that your new Neo4j graph is running.\n",
    "\n",
    "You can download and install Neo4j from here if you haven't already: https://neo4j.com/download/. I will be demonstrating the graph during the class so there's no need to have it installed unless you are also interested in trying out some graph queries yourself.\n",
    "\n",
    "> If you need to build your graph again, make sure to run this cell before running subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "\n",
    "GRAPH_PASSWORD = \"password\" # Set this to the password of your Neo4J graph\n",
    "\n",
    "graph = Graph(password = GRAPH_PASSWORD)\n",
    "\n",
    "# TODO: create an 'id' such as pump__Item so that you can have the same phrase, but different classes\n",
    "\n",
    "# We will start by deleting all nodes and edges in the current graph.\n",
    "# If we don't do this, we will end up with duplicate nodes and edges when running this script again.\n",
    "graph.delete_all() \n",
    "\n",
    "tx = graph.begin()\n",
    "\n",
    "# We will keep a dictionary of nodes that we have created so far.\n",
    "# This serves two purposes:\n",
    "#  - prevents duplicate nodes\n",
    "#  - provides us with a way to create edges between the nodes\n",
    "created_entity_nodes = {}\n",
    "\n",
    "# Creates a node for the specified ngram and entity_class.\n",
    "# If the node has already been created (i.e. it exists in created_nodes), return the node.\n",
    "# Otherwise, create a new one.\n",
    "def create_entity_node(ngram, entity_class):\n",
    "    if ngram in created_entity_nodes:\n",
    "        node = created_entity_nodes[ngram]\n",
    "    else:\n",
    "        node = Node(\"Entity\", entity_class, name=ngram)\n",
    "        created_entity_nodes[ngram] = node\n",
    "        tx.create(node)\n",
    "    return node\n",
    "\n",
    "\n",
    "# Create a node for each triple in the list of triples.\n",
    "# Set the class of each node to the entity_class (e.g. \"activity\", \"item\" or \"observation\").\n",
    "# Create a relationship between the nodes in the triple.\n",
    "for ((ngram_1, entity_class_1), relation, (ngram_2, entity_class_2)) in triples:\n",
    "    \n",
    "    node_1 = create_entity_node(ngram_1, entity_class_1)\n",
    "    node_2 = create_entity_node(ngram_2, entity_class_2)   \n",
    "    \n",
    "    \n",
    "    # Create a relationship between two nodes.\n",
    "    # This does not check for duplicate relationships unlike create_node,\n",
    "    # so this code will need to be adjusted on larger datasets.\n",
    "    relationship = Relationship( node_1, relation, node_2 )\n",
    "    tx.create(relationship)\n",
    "    \n",
    "    \n",
    "tx.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Create nodes for the documents (i.e. the Work Orders)\n",
    "\n",
    "In order to query our graph, we need to create nodes for each work order in our dataset as well. We then need to link each Document node to every Entity node appearing in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "# Our work_order_data and normalised_work_order entities allow us to do this quite easily,\n",
    "\n",
    "tx = graph.begin()\n",
    "\n",
    "# We will once again keep a mapping of created work order nodes, this time indexed by the row index.\n",
    "created_work_order_nodes = {}\n",
    "\n",
    "# Dates are a little awkward in Neo4j - we have to convert it to an integer representation in Python.\n",
    "# The APOC library has functions to handle this better.\n",
    "def date_to_int(date):\n",
    "    parsed_date = parse_date(str(date))\n",
    "    date = int(\"%s%s%s\" % (parsed_date.year, str(parsed_date.month).zfill(2), str(parsed_date.day).zfill(2)))\n",
    "    return date\n",
    "\n",
    "# The process of creating a work order node is a bit different to creating an entity,\n",
    "# as we also want to incorporate some of the structured fields onto the node.\n",
    "def create_structured_node(index, row, node_type, created_nodes):\n",
    "    if index in created_nodes:\n",
    "        return created_nodes[index]\n",
    "\n",
    "    if 'StartDate' in row:\n",
    "        row['StartDate'] = date_to_int(row['StartDate'])\n",
    "    if 'EndDate' in row:\n",
    "        row['EndDate'] = date_to_int(row['EndDate'])  \n",
    "\n",
    "    node = Node(node_type, **row)\n",
    "    created_nodes[index] = node\n",
    "    tx.create(node)\n",
    "    return node\n",
    "\n",
    "for i, row in enumerate(work_order_data):\n",
    "    node = create_structured_node(i, row, \"WorkOrder\", created_work_order_nodes)\n",
    "    \n",
    "tx.commit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Link the entities to their corresponding work order nodes\n",
    "\n",
    "In order to properly query our graph, we need to link every entity node to the work order node in which it appears.\n",
    "\n",
    "This allows us to run queries such as \"pumps with electrical issues in the last 3 months\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "# We can use the normalised_work_order_entries list to do this.\n",
    "for i, row in enumerate(normalised_work_order_entities):\n",
    "    for (ngram, entity_class) in row:        \n",
    "        \n",
    "        node_1 = created_entity_nodes[ngram]\n",
    "        node_2 = created_work_order_nodes[i]\n",
    "        \n",
    "        relationship = Relationship( node_1, \"APPEARS_IN\", node_2 )\n",
    "        tx.create(relationship)\n",
    "       \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Querying the graph\n",
    "\n",
    "## TODO: Update with GQVis\n",
    "\n",
    "Now that the graph has been created, we can query it in Neo4j. This section lists some example queries that we can run on our graph. If you would like to try these yourself you can paste them directly into the Neo4j console.\n",
    "\n",
    "First, let's try a simple query. Here is a query that searches for __all failure modes observed on engines__:\n",
    "\n",
    "    MATCH (e:Entity {name: \"engine\"})-[r:HAS_OBSERVATION]->(o:observation)\n",
    "    RETURN e, r, o\n",
    "\n",
    "We can also use our graph as a way to quickly search and access work orders for the entities appearing in those work orders. For example, searching for __all work orders containing a leak__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(o:observation {name: \"leak\"})\n",
    "    RETURN d, a, o\n",
    "\n",
    "We could extend this to also show the items on which the leaks were present:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(o:observation {name: \"leak\"})<-[r:HAS_OBSERVATION]-(e:Entity)\n",
    "    RETURN d, a, o, r, e\n",
    "\n",
    "Our queries can also incorporate structured data, such as the start dates of the work orders. Here is an example query for __all assets that had leaks from 25 to 28 July__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_OBSERVATION]->(o:observation {name: \"leak\"})-[:APPEARS_IN]->(d)\n",
    "    WHERE d.StartDate >= 20050725\n",
    "    AND d.StartDate <= 20050728\n",
    "    RETURN e, r, o\n",
    "\n",
    "On a larger graph this would also work well with other forms of structured data such as costs. We could query based on specific asset costs, for example.\n",
    "\n",
    "Now that our work orders and downtime events are in one graph, we can also make queries about downtime events. Here is an example query for the __downtime events associated with assets appearing in work orders from 25 to 28 July (where the downtime events occurred in July)__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "We can of course extend this to specific assets, such as pumps:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity {name: \"pump\"})-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "In larger graphs the downtime events could even be further queried based on duration, cost, lost feed, or date ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Future improvements\n",
    "\n",
    "TODO: Get rid of reference to 'downtime'. Perhaps introduce Seq2KG and other models here as alternate ways of text to graph\n",
    "\n",
    "## Incorporating FLOCs\n",
    "\n",
    "Our downtime events are currently linked to Item nodes, but it would make more sense to link them to nodes representing the functional locations.\n",
    "\n",
    "If you are interested in continuing work on this small graph, the next best step would be to create nodes for the functional location data (`floc_data`) and to link the downtime events to those nodes as opposed to the Item nodes.\n",
    "\n",
    "![alt text](images/adding-flocs.png \"Adding FLOCs\")\n",
    "\n",
    "## Frequencies on edge properties\n",
    "\n",
    "We could also improve the graph by incorporating frequencies onto the edge properties. For example, if a \"leak\" occurred on a pump in two different work orders, our link between \"pump\" and \"leak\" could have a property called `frequency` with a value of `2`. This would allow us to query, for example, assets that had a particularly high number of leaks.\n",
    "\n",
    "\n",
    "## Constructing a graph from your own work order data\n",
    "\n",
    "If you have a work order dataset of your own, feel free to download this code and try it out on your dataset. I would be happy to chat if you would like to further discuss the code or if you run into any issues.\n",
    "\n",
    "If you need to extract entities not listed in the lexicon, you will need to update the lexicon file to include your new entities. Alternatively, the LexiconTagger can be substituted for a named entity recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_file = \"data/sample_flocs.csv\"\n",
    "floc_data = load_csv(floc_file)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "## OLD Normalise the entities\n",
    "\n",
    "### MS: Probably going to take this out to save time, I can leave it as a future step for people. It should probably go before NER/RE anyway\n",
    "\n",
    "The next step is to normalise the ngrams, i.e. convert each ngram into a normalised form. This is important as we would prefer to have a single node for a single concept, e.g. one node for \"engine\" as opposed to two nodes for \"engin\" and \"engine\".\n",
    "\n",
    "We will once again be using a lexicon for this task, but it would typically be performed by machine learning.\n",
    "\n",
    "![alt text](images/normalising-entities.png \"Normalising entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_n_file = \"data/lexicon_normalisation.csv\"\n",
    "lexicon_normaliser = LexiconTagger(lexicon_n_file)\n",
    "\n",
    "normalised_work_order_entities = []\n",
    "\n",
    "# For every row in work_order_entities, replace each ngram with its normalised counterpart\n",
    "# as per the normalisation lexicon.\n",
    "# For example, \"engin\" will become \"engine\", \"leaking\" will become \"leak\", etc.\n",
    "for row in work_order_entities:\n",
    "    normalised_work_order_entities.append([(lexicon_normaliser.normalise_ngram(ngram), entity_class) \n",
    "                                           for (ngram, entity_class) in row])\n",
    "    \n",
    "    \n",
    "for row in normalised_work_order_entities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Extending the graph to incorporate Downtime events\n",
    "\n",
    "The next step is to incorporate the downtime events.\n",
    "\n",
    "For this exercise we are going to link the Downtime events to the first Item node appearing in the work orders with the same FLOC as the downtime event.\n",
    "\n",
    "\n",
    "![alt text](images/adding-downtime-events.png \"Adding downtime events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "created_downtime_nodes = {}\n",
    "\n",
    "# Create a DowntimeEvent node for each row\n",
    "for i, downtime_row in enumerate(downtime_data):\n",
    "    node = create_structured_node(i, downtime_row, \"DowntimeEvent\", created_downtime_nodes)\n",
    "    \n",
    "    # Get all work order nodes with the same FLOC and link the DowntimeEvent to the Items appearing\n",
    "    # in those work orders\n",
    "    for j, work_order_row in enumerate(work_order_data):\n",
    "        if work_order_row[\"FLOC\"] == downtime_row[\"FLOC\"]:\n",
    "            \n",
    "            work_order_entities = normalised_work_order_entities[j]\n",
    "            \n",
    "            for (ngram, entity_class) in work_order_entities:\n",
    "                if entity_class != \"item\": continue    # We don't need to link non-items to downtime events               \n",
    "                    \n",
    "                item_node = created_entity_nodes[ngram]\n",
    "                relationship = Relationship( item_node, \"HAS_EVENT\", node )\n",
    "                tx.create(relationship)\n",
    "                break\n",
    "\n",
    "    \n",
    "tx.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
